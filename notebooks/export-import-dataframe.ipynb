{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Опыты по сохранению и восстановлению данных из датафреймов\n",
    "\n",
    "Экспорт импорт из и в базы данных и CSV файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages installed in local directories\n",
    "import os\n",
    "import sys\n",
    "google_drive_path = '/drive/My Drive/Colab Notebooks'\n",
    "packages_dir = os.getcwd()+'/packages'\n",
    "sys.path.append(packages_dir)\n",
    "# !pip install psycopg2-binary --target=$packages_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sysв'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d6923110737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# import gzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msysв\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mRGDSN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGDSN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sysв'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import sqlite3\n",
    "import psycopg2.extras  \n",
    "import pandas as pd\n",
    "import time\n",
    "# import gzip\n",
    "import zlib\n",
    "import sysв\n",
    "RGDSN = os.getenv('RGDSN')\n",
    "\n",
    "class ATimer:\n",
    "    \"\"\"A utility class to mesure time between sequential calls.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.t = time.time()\n",
    "    def show(self, message ='', end='\\n'):\n",
    "        \"\"\"Prints a string appended with time lapsed from the last call\"\"\"\n",
    "        t1 = time.time()\n",
    "        print(f'{message} in {t1 - self.t:.3f} sec.', end=end)\n",
    "        self.t = t1\n",
    "\n",
    "\n",
    "def dataframe_from_sql(sql):\n",
    "    t= ATimer()\n",
    "    con = psycopg2.connect(RGDSN)\n",
    "    t.show('Connected', end=' ')\n",
    "    df = pd.read_sql_query(sql, con)\n",
    "    t.show('Executed', end=' ')\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def compress(s: str)-> bytes:\n",
    "    \"compresses string s into byte array\"\n",
    "    return zlib.compress(s.encode('utf-8'))\n",
    "\n",
    "def decompress(c: bytearray) -> str:\n",
    "    return zlib.decompress(c).decode('utf-8')\n",
    "\n",
    "def show_size(df):\n",
    "    \"Prints size of df  in Mb\"\n",
    "    size = sys.getsizeof(df)/1024/1024\n",
    "    # size = df.memory_usage(index=True, deep=True).sum()/1024/1024\n",
    "    print(f'The size is {size :.2f} Mb')\n",
    "    \n",
    "def sql_queries(sql:str, limit=3, max_offset=10, start_offset=0 ):\n",
    "    \"Generates sql queries adding LIMIT and OFFSET\"\n",
    "    for i in range(start_offset,max_offset,limit):\n",
    "        yield f'{sql} LIMIT {limit} OFFSET {i}'\n",
    "        \n",
    "def accumulate(df):\n",
    "    accumulate.sum = pd.concat([accumulate.sum, df], ignore_index=True, copy=True)\n",
    "accumulate.sum = pd.DataFrame()\n",
    "\n",
    "def execute_sql_queries( sql:str, func, limit=3, max_offset=10, start_offset=0 ):\n",
    "    \"\"\"Выполняет серию SQL запросов. Над результатом каждого выполняет функцию func.\"\"\"\n",
    "    t = ATimer()\n",
    "    for sql in sql_queries(sql, limit=limit, max_offset=max_offset, start_offset=start_offset):\n",
    "        df = dataframe_from_sql(sql)\n",
    "        if callable(func):\n",
    "            func(df)\n",
    "        t.show(f'{len(accumulate.sum)} records added')\n",
    "        show_size(accumulate.sum)\n",
    "\n",
    "# https://stackoverflow.com/questions/39100971/how-do-i-release-memory-used-by-a-pandas-dataframe\n",
    "import psutil\n",
    "def usage():\n",
    "    p = psutil.Process(os.getpid())\n",
    "    return p.memory_info().rss / float(2 ** 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accumulate.sum = pd.DataFrame()\n",
    "\n",
    "# tt = ATimer()\n",
    "\n",
    "# execute_sql_queries(\n",
    "#     'SELECT obj_id, lemmatized_text FROM articles ORDER BY obj_id', \n",
    "#     accumulate, \n",
    "#     limit=10000, \n",
    "#     max_offset=1210000, \n",
    "#     start_offset=0)    \n",
    "\n",
    "# print('------------------------------')\n",
    "# tt.show('finished')\n",
    "# show_size(accumulate.sum)\n",
    "# # print(f'{sys.getsizeof(accumulate.sum)/1024/1024:.2f} Mb ')\n",
    "# display(accumulate.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install snappy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "Saving to csv takes 5 min (ssd 1min 26s, 1min 24s), to csv.zip 12 min. To SQLite3 107sec (ssd 1min 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# accumulate.sum.to_parquet('df.parquet.gzip', compression='gzip')  \n",
    "# accumulate.sum.to_parquet('df.parquet')\n",
    "df.to_csv('/Volumes/ssd/dumps/articles_i3.csv')\n",
    "# accumulate.sum.to_pickle('accumulate.sum.pkl')\n",
    "# accumulate.sum.to_csv('accumulate.sum.csv.zip')\n",
    "\n",
    "# con = sqlite3.connect('/Volumes/ssd/dumps/df1.db')\n",
    "# df.to_sql('art',con)\n",
    "# con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data\n",
    "==========\n",
    "\n",
    "**5067.48 Mb. 1202159 records.**\n",
    "\n",
    "type  | time\n",
    "----  |----\n",
    "csv   | 1 min (ssd 52s, 48s)\n",
    "csv.gz| 1 min 30s (ssd 1 min 11s)\n",
    "**sqlite** pd.read_sql_table('art','sqlite:///df.db')| 3 min\n",
    "**sqlite3** pd.read_sql_query('select * from art',con) | 1 min 52s, 2min 10s (ssd 3min 11s, 3min 9s)\n",
    "**postgresql** pd.read_sql_query('SELECT obj_id, lemmatized_text FROM articles',con)| 5 min 30s.  CPU times: user 26.4 s, sys: 52.7 s, total: 1min 19s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.8 s, sys: 6.07 s, total: 48.9 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# CSV\n",
    "df = pd.read_csv('/Volumes/ssd/dumps/articles_i.csv')\n",
    "\n",
    "# SQLite\n",
    "# con = sqlite3.connect('./df.db')\n",
    "# con = sqlite3.connect('/Volumes/ssd/dumps/df.db')\n",
    "# df = pd.read_sql_query('select * from art',con)\n",
    "\n",
    "# Postresql\n",
    "# con = psycopg2.connect(os.getenv('RGDSN'))\n",
    "# df = pd.read_sql_query('SELECT obj_id, lemmatized_text FROM articles',con)\n",
    "\n",
    "# con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num processed rows= 1202159\n",
      "CPU times: user 3.77 s, sys: 49.5 ms, total: 3.82 s\n",
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "v = ''\n",
    "c = 0\n",
    "\n",
    "def incc(row):\n",
    "    global c\n",
    "    global v\n",
    "#     v = row['lemmatized_text']\n",
    "    c+=1\n",
    "    \n",
    "# for index, row in df.iterrows():\n",
    "# #     v = row['lemmatized_text']\n",
    "#     incc()\n",
    "\n",
    "df.apply(lambda row: incc(row), axis=1)\n",
    "\n",
    "print(f'num processed rows= {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1202159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count\n",
       "0  1202159"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "con = psycopg2.connect(os.getenv('RGDSN'))\n",
    "df = pd.read_sql_query(\"select count(1) from articles\", con )\n",
    "con.close()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipped in 1.484 sec.\n"
     ]
    }
   ],
   "source": [
    "t = ATimer()\n",
    "# df['len']=df.lemmatized_text.str.len()\n",
    "df['z']=df['lemmatized_text'].apply(compress)\n",
    "# df['lenz']=df['z'].str.len()\n",
    "t.show('zipped')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.423781394958496"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: \n",
    "    df.drop(columns=['lemmatized_text'], inplace=True)\n",
    "except: \n",
    "    pass\n",
    "df.memory_usage(deep=True).sum()/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.show('begin')\n",
    "df['restored'] = df.z.apply(decompress)\n",
    "t.show('end')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://jonathansoma.com/lede/foundations/classes/pandas%20columns%20and%20functions/apply-a-function-to-every-row-in-a-pandas-dataframe/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
